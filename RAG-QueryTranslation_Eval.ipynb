{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8eb9b1",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92479c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- dependencies  ---\n",
    "from dotenv import load_dotenv\n",
    "import os, json, math, statistics, uuid, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict, OrderedDict\n",
    "from functools import lru_cache\n",
    "\n",
    "# LangChain + community\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "# Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabaa2b7",
   "metadata": {},
   "source": [
    "Loads common libs.\n",
    "\n",
    "load_dotenv pulls env vars from .env.\n",
    "\n",
    "Path for file paths; typing for type hints.\n",
    "\n",
    "defaultdict/OrderedDict help with scoring & result ordering.\n",
    "\n",
    "lru_cache caches function outputs (for speed).\n",
    "\n",
    "PDF loader → turns PDFs into Documents.\n",
    "\n",
    "Splitter → breaks docs into chunks for vector search.\n",
    "\n",
    "OllamaEmbeddings → local embedding model.\n",
    "\n",
    "FAISS → vector index for similarity search.\n",
    "\n",
    "Prompt templating and string parsing for LLM I/O.\n",
    "\n",
    "dumps/loads used for (de)duping docs by serializing.\n",
    "\n",
    "The Groq-hosted LLM wrapper (generator + judge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff38c83",
   "metadata": {},
   "source": [
    "#  ENV & CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8c58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K = 4                \n",
    "N_QUERIES = 5            \n",
    "\n",
    "# Models\n",
    "GEN_MODEL = \"llama3-8b-8192\"        \n",
    "JUDGE_MODEL = \"llama3-8b-8192\"   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b388f2",
   "metadata": {},
   "source": [
    "Loads the API key at runtime; asserts it’s present.\n",
    "Tuning knobs: where PDFs are, how to chunk, how many chunks to retrieve (TOP_K), and how many reformulated queries to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64e199",
   "metadata": {},
   "source": [
    "LOAD DOCS & BUILD / LOAD INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7a5785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_20464\\1956189531.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "def load_corpus_and_index() -> Tuple[List[Document], FAISS]:\n",
    "    pdf_paths = list(Path(DATA_DIR).glob(\"*.pdf\"))\n",
    "    docs: List[Document] = []\n",
    "    for path in pdf_paths:\n",
    "        loader = PyPDFLoader(str(path))\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"Total Chunks: {len(chunks)}\")\n",
    "\n",
    "    embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    if not Path(INDEX_DIR).exists():\n",
    "        db = FAISS.from_documents(chunks, embedding=embedding)\n",
    "        db.save_local(INDEX_DIR)\n",
    "\n",
    "    db = FAISS.load_local(INDEX_DIR, embeddings=embedding, allow_dangerous_deserialization=True)\n",
    "    return chunks, db\n",
    "\n",
    "corpus_chunks, faiss_db = load_corpus_and_index()\n",
    "retriever = faiss_db.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a349f",
   "metadata": {},
   "source": [
    "Defines a function to read all PDFs from data/ and turn each page into Documents. \n",
    "Splits docs into overlapping chunks (improves retrieval granularity).\n",
    "Creates embeddings and builds a FAISS index if it doesn’t already exist, then saves it.\n",
    "Loads (or just created) FAISS index; returns both the chunk list and the FAISS store.\n",
    "Calls load_corpus_and_index() and sets up a retriever wrapper (always fetches k=TOP_K chunks).\n",
    "\n",
    "Also instantiates a (reusable) embedding model for other computations (e.g., eval metric).\n",
    "\n",
    "An index is needed to do vector similarity search; this block ensures it exists and exposes a simple retriever for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b5a93",
   "metadata": {},
   "source": [
    "2) LLMs (generator + judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edb47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=GEN_MODEL, temperature=0.0)\n",
    "judge_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=JUDGE_MODEL, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cf377",
   "metadata": {},
   "source": [
    "Two model roles: generator (answers questions) and judge (scores answers).\n",
    "Two LLM clients: one to generate answers; another to score them.\n",
    "\n",
    "temperature=0.0 for deterministic-ish outputs and stable scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f23096",
   "metadata": {},
   "source": [
    "3) UTILS (caching, formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=4096)\n",
    "def embed_query_cached(text: str) -> List[float]:\n",
    "    return embedding.embed_query(text)\n",
    "\n",
    "def embed_documents(texts: List[str]) -> List[List[float]]:\n",
    "    # small manual cache to reduce duplicate doc embeddings\n",
    "    cache: Dict[str, List[float]] = {}\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        if t not in cache:\n",
    "            cache[t] = embedding.embed_query(t)  # for FAISS, doc/query vector in same space\n",
    "        out.append(cache[t])\n",
    "    return out\n",
    "\n",
    "def cosine(a: List[float], b: List[float]) -> float:\n",
    "    dot = sum(x*y for x, y in zip(a, b))\n",
    "    na = math.sqrt(sum(x*x for x in a))\n",
    "    nb = math.sqrt(sum(y*y for y in b))\n",
    "    return 0.0 if na == 0 or nb == 0 else dot / (na * nb)\n",
    "\n",
    "def docs_to_context(docs: List[Document]) -> str:\n",
    "    parts = []\n",
    "    for i, d in enumerate(docs[:TOP_K]):\n",
    "        meta = d.metadata or {}\n",
    "        src = meta.get(\"source\", \"\")\n",
    "        page = meta.get(\"page\", \"\")\n",
    "        header = f\"[DOC {i+1} | {Path(src).name if src else 'unknown'} | page {page}]\"\n",
    "        body = d.page_content or \"\"\n",
    "        parts.append(f\"{header}\\n{body}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def serialize_doc_id(doc: Document) -> str:\n",
    "    # stable identity for fusion/dedup\n",
    "    meta = doc.metadata or {}\n",
    "    key = f\"{meta.get('source','')}-{meta.get('page','')}-{hash(doc.page_content)}\"\n",
    "    return key\n",
    "\n",
    "def dedupe_docs_preserve_order(docs: List[Document]) -> List[Document]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        sid = serialize_doc_id(d)\n",
    "        if sid not in seen:\n",
    "            seen.add(sid)\n",
    "            out.append(d)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0260bf",
   "metadata": {},
   "source": [
    "Caches query embeddings so repeated calls don’t recompute (speeds up eval).\n",
    "Embeds a list of texts with tiny per-call caching (reduces duplicate work within the call).\n",
    "Plain cosine similarity for vectors (used in retrieval relevance metric).\n",
    "Converts retrieved docs into a readable context string for the generator prompt (includes [DOC i | source | page] headers).\n",
    "Creates a stable-ish key for a chunk to help dedup across different retrieval strategies.\n",
    "Removes duplicate doc chunks while preserving order so as to keep top-ranked items first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b9d0e",
   "metadata": {},
   "source": [
    " 4) QUERY TRANSLATION TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_Q_TEMPLATE = \"\"\"Generate {n} distinct reformulations of this question to help vector retrieval.\n",
    "Return one per line, no numbering.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "DECOMP_TEMPLATE = \"\"\"Decompose the following question into 3-5 concrete sub-questions that, if answered, together answer the original.\n",
    "Return one per line, no numbering.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "STEPBACK_TEMPLATE = \"\"\"Rewrite the question into a more general, step-back abstraction that captures the core goal concisely.\n",
    "Return only the abstracted question, no commentary.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "HYDE_TEMPLATE = \"\"\"Write a short, factual hypothetical answer to the question below (even if you don't actually know).\n",
    "Keep it 3-5 sentences, neutral tone, no sources.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "def gen_lines(prompt_tmpl: str, **kwargs) -> List[str]:\n",
    "    p = ChatPromptTemplate.from_template(prompt_tmpl)\n",
    "    chain = p | llm | StrOutputParser()\n",
    "    raw = chain.invoke(kwargs)\n",
    "    lines = [ln.strip() for ln in raw.split(\"\\n\") if ln.strip()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea16f8f",
   "metadata": {},
   "source": [
    "These strings prompt the LLM to produce alternative queries used by the different strategies.\n",
    "gen_lines defines a helper to:\n",
    "\n",
    "build a prompt from a template,\n",
    "\n",
    "call the generator LLM,\n",
    "\n",
    "split into one-line outputs (list of strings).\n",
    "\n",
    "Used by all retrieval strategies that need multiple queries (multi, fusion, decomposition, step-back, hyde)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314bbf9",
   "metadata": {},
   "source": [
    "5) RETRIEVAL STRATEGIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c1cab",
   "metadata": {},
   "source": [
    "Each strategy returns a list of Documents (the top retrieval set). They’re later fed to the generator to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b790e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_base(question: str, k: int = TOP_K) -> List[Document]:\n",
    "    return retriever.get_relevant_documents(question)[:k]\n",
    "\n",
    "def retrieve_multi_query(question: str, n: int = N_QUERIES, k: int = TOP_K) -> List[Document]:\n",
    "    alts = gen_lines(MULTI_Q_TEMPLATE, n=n, question=question)\n",
    "    docs: List[Document] = []\n",
    "    for q in alts:\n",
    "        docs.extend(retriever.get_relevant_documents(q))\n",
    "    return dedupe_docs_preserve_order(docs)[:max(k, len(docs))][:k]\n",
    "\n",
    "def retrieve_rag_fusion(question: str, n: int = N_QUERIES, k: int = TOP_K, rrf_k:int=60) -> List[Document]:\n",
    "    \"\"\"Reciprocal Rank Fusion over results from multiple queries.\"\"\"\n",
    "    alts = gen_lines(MULTI_Q_TEMPLATE, n=n, question=question)\n",
    "    rankings: List[List[Document]] = []\n",
    "    for q in alts:\n",
    "        rankings.append(retriever.get_relevant_documents(q))\n",
    "\n",
    "    scores: Dict[str, float] = defaultdict(float)\n",
    "    doc_by_id: Dict[str, Document] = {}\n",
    "    for rank_list in rankings:\n",
    "        for r, d in enumerate(rank_list, start=1):\n",
    "            did = serialize_doc_id(d)\n",
    "            scores[did] += 1.0 / (rrf_k + r)\n",
    "            doc_by_id[did] = d\n",
    "\n",
    "    # order by fused score\n",
    "    ordered = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    fused_docs = [doc_by_id[did] for did, _ in ordered]\n",
    "    return fused_docs[:k]\n",
    "\n",
    "def retrieve_decomposition(question: str, k: int = TOP_K) -> List[Document]:\n",
    "    subs = gen_lines(DECOMP_TEMPLATE, question=question)\n",
    "    docs: List[Document] = []\n",
    "    for sq in subs:\n",
    "        docs.extend(retriever.get_relevant_documents(sq))\n",
    "    # Prioritize documents appearing across many sub-queries\n",
    "    counts: Dict[str, int] = defaultdict(int)\n",
    "    by_id: Dict[str, Document] = {}\n",
    "    for d in docs:\n",
    "        did = serialize_doc_id(d)\n",
    "        counts[did] += 1\n",
    "        by_id[did] = d\n",
    "    ordered = sorted(counts.items(), key=lambda x: (x[1], len(by_id[x[0]].page_content)), reverse=True)\n",
    "    return [by_id[did] for did, _ in ordered][:k]\n",
    "\n",
    "def retrieve_step_back(question: str, k: int = TOP_K) -> List[Document]:\n",
    "    abstract_qs = gen_lines(STEPBACK_TEMPLATE, question=question)\n",
    "    abstract = abstract_qs[0] if abstract_qs else question\n",
    "    combined_docs = retriever.get_relevant_documents(question) + retriever.get_relevant_documents(abstract)\n",
    "    return dedupe_docs_preserve_order(combined_docs)[:k]\n",
    "\n",
    "def retrieve_hyde(question: str, k: int = TOP_K) -> List[Document]:\n",
    "    hyp = gen_lines(HYDE_TEMPLATE, question=question)\n",
    "    hyp_text = hyp[0] if hyp else question\n",
    "    # Use hypothetical answer as retrieval query (optionally mix with original)\n",
    "    docs = retriever.get_relevant_documents(hyp_text) + retriever.get_relevant_documents(question)\n",
    "    return dedupe_docs_preserve_order(docs)[:k]\n",
    "\n",
    "RETRIEVERS = OrderedDict({\n",
    "    \"multi_query\": retrieve_multi_query,\n",
    "    \"rag_fusion\": retrieve_rag_fusion,\n",
    "    \"decomposition\": retrieve_decomposition,\n",
    "    \"step_back\": retrieve_step_back,\n",
    "    \"hyde\": retrieve_hyde,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfaae60",
   "metadata": {},
   "source": [
    "Simple baseline .\n",
    "Retrieve Multi- Query : Generate N query variants, retrieve for each, concat all hits, dedupe, then take top k.\n",
    "Retrieve RAG Fusion: run multi queries, then apply Reciprocal Rank Fusion.\n",
    "Each doc gets a higher score if it appears high across many lists; final top k returned.\n",
    "Decomposition: break the question into sub-questions.Rank docs by how often they appear across sub-queries (tie-breaker: longer content).Return top k.\n",
    "Step-back: generate a more abstract version of the question; retrieve for both the original and the abstract; merge & dedupe, then top k.\n",
    "HyDE: generate a hypothetical answer first, use it as a query to retrieve, also add retrieval on original, merge & dedupe, top k.\n",
    "A registry mapping names → functions.\n",
    "\n",
    "RETRIEVERS is Used later by the evaluation loop to run all strategies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db297",
   "metadata": {},
   "source": [
    "6) RAG ANSWER PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1426e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"Answer the following question based ONLY on this context. If the answer\n",
    "is not contained in the context, say \"I don't have enough information in the provided context.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "# Correct wiring: extract fields explicitly\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def generate_answer(question: str, docs: List[Document]) -> Tuple[str, str]:\n",
    "    context_str = docs_to_context(docs)\n",
    "    answer = rag_chain.invoke({\"context\": context_str, \"question\": question})\n",
    "    return answer, context_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86472b35",
   "metadata": {},
   "source": [
    "RAG_TEMPLATE: Defines the instruction the generator uses to answer from context only.\n",
    "rag_chain is A LangChain runnable pipeline:\n",
    "itemgetter extracts \"context\" and \"question\" fields from the input dict,\n",
    "fills rag_prompt,\n",
    "calls the generator (llm),\n",
    "parses the output to a string.\n",
    "generate_answer: Called in the evaluation loop:\n",
    "Converts retrieved docs → context string,\n",
    "Runs the RAG chain to get the model answer,\n",
    "Returns both the answer and the context used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055aede5",
   "metadata": {},
   "source": [
    "7) EVAL: LLM-AS-JUDGE PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c16ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTNESS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are grading an answer for correctness against a reference solution.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Model Answer:\n",
    "{answer}\n",
    "\n",
    "Reference Answer (the ground truth):\n",
    "{reference}\n",
    "\n",
    "Return strict JSON: {{\"score\": <1-5 integer>, \"justification\": \"<=30 words\"}}\n",
    "Only output JSON.\n",
    "\"\"\".strip())\n",
    "\n",
    "GROUNDED_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are grading how well the answer is supported by the provided context (groundedness).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Model Answer:\n",
    "{answer}\n",
    "\n",
    "Grade 1-5 (5 = fully supported, 1 = not supported). Penalize claims not in context.\n",
    "Return JSON: {{\"score\": <1-5 integer>, \"justification\": \"<=30 words\"}}\n",
    "Only output JSON.\n",
    "\"\"\".strip())\n",
    "\n",
    "RELEVANCE_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are grading answer relevance to the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Model Answer:\n",
    "{answer}\n",
    "\n",
    "Grade 1-5 (5 = directly answers; 1 = off-topic).\n",
    "Return JSON: {{\"score\": <1-5 integer>, \"justification\": \"<=30 words\"}}\n",
    "Only output JSON.\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e25ac8",
   "metadata": {},
   "source": [
    "These templates instruct the judge model on how to score.Each asks for strict JSON with a 1–5 score and a short justification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2648d97",
   "metadata": {},
   "source": [
    " 7)Judge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84786757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_safe(text: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        return json.loads(text.strip())\n",
    "    except Exception:\n",
    "        s = text.rfind(\"{\"); e = text.rfind(\"}\")\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            try:\n",
    "                return json.loads(text[s:e+1])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {\"score\": None, \"justification\": \"parse_error\"}\n",
    "\n",
    "def judge_correctness(q: str, a: str, ref: str) -> Dict[str, Any]:\n",
    "    out = CORRECTNESS_PROMPT | judge_llm | StrOutputParser()\n",
    "    res = out.invoke({\"question\": q, \"answer\": a, \"reference\": ref})\n",
    "    return parse_json_safe(res)\n",
    "\n",
    "def judge_groundedness(q: str, a: str, ctx: str) -> Dict[str, Any]:\n",
    "    out = GROUNDED_PROMPT | judge_llm | StrOutputParser()\n",
    "    res = out.invoke({\"question\": q, \"answer\": a, \"context\": ctx})\n",
    "    return parse_json_safe(res)\n",
    "\n",
    "def judge_relevance(q: str, a: str) -> Dict[str, Any]:\n",
    "    out = RELEVANCE_PROMPT | judge_llm | StrOutputParser()\n",
    "    res = out.invoke({\"question\": q, \"answer\": a})\n",
    "    return parse_json_safe(res)\n",
    "\n",
    "def retrieval_relevance_embedding(question: str, docs: List[Document]) -> float:\n",
    "    qv = embed_query_cached(question)\n",
    "    if not docs:\n",
    "        return 0.0\n",
    "    texts = [d.page_content or \"\" for d in docs]\n",
    "    dvs = embed_documents(texts)\n",
    "    sims = [cosine(qv, dv) for dv in dvs]\n",
    "    return float(sum(sims)/len(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c66ee",
   "metadata": {},
   "source": [
    "parse_json_safe: Tries to parse judge outputs as JSON; has a fallback that extracts the last {...} block if the model added extra text.\n",
    "judge_correctness: Called per example: sends question, model answer, and reference answer to the judge.\n",
    "judge_groundedness: Called per example: checks if the answer is supported by the retrieved context only.\n",
    "judge_relevance: Called per example: evaluates how on-topic the answer is to the question.\n",
    "retrieval_relevance_embedding: Embedding-based retrieval relevance metric in [0..1]: average cosine between question and retrieved chunks.\n",
    "Called per example: gives a numeric signal of how well the retriever matched the question.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d41aa",
   "metadata": {},
   "source": [
    "8) Dataset of Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76dab2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "monopoly_dataset = [\n",
    "    {\"question\": \"What happens when a player lands on an unowned property?\",\n",
    "     \"answer\": \"The player may buy it from the Bank at its printed price. If they don’t buy it, the property is auctioned to the highest bidder.\"},\n",
    "    {\"question\": \"Can you collect rent while in Jail?\",\n",
    "     \"answer\": \"Yes, a player may collect rent while in Jail.\"},\n",
    "    {\"question\": \"What are the three ways to get out of Jail?\",\n",
    "     \"answer\": \"Rolling doubles, using a 'Get Out of Jail Free' card, or paying $50 before your next turn.\"},\n",
    "    {\"question\": \"What happens when you land on or pass GO?\",\n",
    "     \"answer\": \"You collect $200 from the Bank.\"},\n",
    "    {\"question\": \"How is rent affected by owning all properties in a color group?\",\n",
    "     \"answer\": \"Rent is doubled on unimproved properties when a player owns all properties in a color group.\"},\n",
    "    {\"question\": \"When can you build houses on your properties?\",\n",
    "     \"answer\": \"You must own all properties in a color group and build evenly, one house at a time on each property.\"},\n",
    "    {\"question\": \"What happens if you don’t have enough cash to pay a debt?\",\n",
    "     \"answer\": \"You must mortgage properties or sell houses/hotels to raise the funds. If you still can't pay, you’re bankrupt.\"},\n",
    "    {\"question\": \"Can a mortgaged property earn rent?\",\n",
    "     \"answer\": \"No, rent cannot be collected on mortgaged properties.\"},\n",
    "    {\"question\": \"What is the cost of each house and hotel?\",\n",
    "     \"answer\": \"Each house and hotel has a price listed on the property deed card.\"},\n",
    "    {\"question\": \"How do you win Monopoly?\",\n",
    "     \"answer\": \"You win by being the last player remaining after all others have gone bankrupt.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19600c6",
   "metadata": {},
   "source": [
    "Boardgame Q/A set for evaluation.\n",
    "Used by the eval loop for every method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc037813",
   "metadata": {},
   "source": [
    "9) EVALUATION LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a1297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_safe(vals):\n",
    "    vals = [v for v in vals if v is not None]\n",
    "    return round(float(statistics.mean(vals)), 3) if vals else None\n",
    "\n",
    "def eval_method(method_name: str, retrieve_fn, dataset: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "    print(f\"\\n=== Evaluating method: {method_name} ===\")\n",
    "    results = []\n",
    "    for i, item in enumerate(dataset, 1):\n",
    "        q = item[\"question\"]; ref = item[\"answer\"]\n",
    "\n",
    "        docs = retrieve_fn(q)\n",
    "        ans, ctx = generate_answer(q, docs)\n",
    "\n",
    "        corr = judge_correctness(q, ans, ref)\n",
    "        grnd = judge_groundedness(q, ans, ctx)\n",
    "        relv = judge_relevance(q, ans)\n",
    "        retr_rel = retrieval_relevance_embedding(q, docs)\n",
    "\n",
    "        rec = {\n",
    "            \"id\": i,\n",
    "            \"method\": method_name,\n",
    "            \"question\": q,\n",
    "            \"reference_answer\": ref,\n",
    "            \"model_answer\": ans,\n",
    "            \"correctness\": corr,\n",
    "            \"groundedness\": grnd,\n",
    "            \"relevance\": relv,\n",
    "            \"retrieval_relevance_embedding\": retr_rel,\n",
    "            \"retrieved_context_preview\": ctx[:1200],\n",
    "        }\n",
    "        results.append(rec)\n",
    "\n",
    "        print(f\"[{i}] Correctness: {corr.get('score')} | Grounded: {grnd.get('score')} | \"\n",
    "              f\"Relevance: {relv.get('score')} | RetrRel: {retr_rel:.3f}\")\n",
    "\n",
    "    summary = {\n",
    "        \"method\": method_name,\n",
    "        \"n\": len(results),\n",
    "        \"avg_correctness_1to5\": avg_safe([r[\"correctness\"].get(\"score\") for r in results]),\n",
    "        \"avg_groundedness_1to5\": avg_safe([r[\"groundedness\"].get(\"score\") for r in results]),\n",
    "        \"avg_relevance_1to5\":   avg_safe([r[\"relevance\"].get(\"score\")   for r in results]),\n",
    "        \"avg_retrieval_relevance_0to1\": avg_safe([r[\"retrieval_relevance_embedding\"] for r in results]),\n",
    "        \"generator_model\": GEN_MODEL,\n",
    "        \"judge_model\": JUDGE_MODEL,\n",
    "    }\n",
    "    return {\"summary\": summary, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ce15c",
   "metadata": {},
   "source": [
    "Helper to average scores safely (ignores None).\n",
    "eval_method: Defines the core evaluation for one method:\n",
    "    Calls that method’s retriever function (retrieve_fn) to get docs.\n",
    "    Calls generate_answer to produce the model’s answer from those docs.\n",
    "    Calls all judge functions to score the answer.\n",
    "    Builds a result record (keeps a preview of context).\n",
    "    Prints a per-item score line.\n",
    "    Computes averages across the dataset and returns both the summary and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19406dad",
   "metadata": {},
   "source": [
    "10) Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52adaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    all_summaries = []\n",
    "    out_dir = Path(f\"rag_eval_{ts}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name, fn in RETRIEVERS.items():\n",
    "        eval_out = eval_method(name, fn, monopoly_dataset)\n",
    "        summary = eval_out[\"summary\"]; results = eval_out[\"results\"]\n",
    "        all_summaries.append(summary)\n",
    "\n",
    "        # Save per-method artifacts\n",
    "        with (out_dir / f\"{name}_results.jsonl\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for r in results:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        with (out_dir / f\"{name}_summary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Combined summary\n",
    "    combined = {\n",
    "        \"methods\": all_summaries,\n",
    "        \"best_by_correctness\": max(all_summaries, key=lambda s: s[\"avg_correctness_1to5\"] or 0.0),\n",
    "        \"best_by_groundedness\": max(all_summaries, key=lambda s: s[\"avg_groundedness_1to5\"] or 0.0),\n",
    "        \"best_by_relevance\":   max(all_summaries, key=lambda s: s[\"avg_relevance_1to5\"] or 0.0),\n",
    "        \"best_by_retrieval_relevance\": max(all_summaries, key=lambda s: s[\"avg_retrieval_relevance_0to1\"] or 0.0),\n",
    "    }\n",
    "    with (out_dir / \"combined_summary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n=== Combined Summary ===\")\n",
    "    print(json.dumps(combined, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71951684",
   "metadata": {},
   "source": [
    "Calls eval_method once per retrieval strategy in the registry, accumulates per-method summaries.\n",
    "Writes detailed per-item results to JSONL and a per-method summary JSON.\n",
    "Aggregates all method summaries, picks best per metric, writes a combined summary JSON, and prints it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde1299",
   "metadata": {},
   "source": [
    "11) Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569d534",
   "metadata": {},
   "source": [
    "Load PDFs → chunk → embed → index (load_corpus_and_index), create a retriever.\n",
    "\n",
    "Define five retrieval strategies that transform the question (multi, fusion, decomposition, step-back, hyde) → return top k chunks.\n",
    "\n",
    "RAG answer pipeline (rag_chain) takes {context, question} → model answer.\n",
    "\n",
    "Judge metrics: correctness vs reference; groundedness vs context; relevance vs question; retrieval relevance by cosine.\n",
    "\n",
    "Evaluate each strategy over every Q/A, print & save per-item results, per-method summary, and a combined “best of” summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9d1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating method: multi_query ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_20464\\534760016.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs.extend(retriever.get_relevant_documents(q))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Correctness: 2 | Grounded: 1 | Relevance: 2 | RetrRel: 0.693\n",
      "[2] Correctness: 2 | Grounded: 1 | Relevance: 2 | RetrRel: 0.566\n",
      "[3] Correctness: 2 | Grounded: 1 | Relevance: 1 | RetrRel: 0.519\n",
      "[4] Correctness: 1 | Grounded: 1 | Relevance: 5 | RetrRel: 0.504\n",
      "[5] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.631\n",
      "[6] Correctness: 2 | Grounded: 2 | Relevance: 2 | RetrRel: 0.674\n",
      "[7] Correctness: 2 | Grounded: None | Relevance: 1 | RetrRel: 0.596\n",
      "[8] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.661\n",
      "[9] Correctness: 1 | Grounded: 1 | Relevance: 5 | RetrRel: 0.564\n",
      "[10] Correctness: 2 | Grounded: 1 | Relevance: 1 | RetrRel: 0.601\n",
      "\n",
      "=== Evaluating method: rag_fusion ===\n",
      "[1] Correctness: 2 | Grounded: 2 | Relevance: 2 | RetrRel: 0.711\n",
      "[2] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.638\n",
      "[3] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.668\n",
      "[4] Correctness: 4 | Grounded: None | Relevance: 5 | RetrRel: 0.588\n",
      "[5] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.665\n",
      "[6] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.704\n",
      "[7] Correctness: 2 | Grounded: 5 | Relevance: 1 | RetrRel: 0.656\n",
      "[8] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.674\n",
      "[9] Correctness: 1 | Grounded: 1 | Relevance: 5 | RetrRel: 0.615\n",
      "[10] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.729\n",
      "\n",
      "=== Evaluating method: decomposition ===\n",
      "[1] Correctness: 3 | Grounded: 5 | Relevance: 5 | RetrRel: 0.692\n",
      "[2] Correctness: 2 | Grounded: 2 | Relevance: 2 | RetrRel: 0.617\n",
      "[3] Correctness: 2 | Grounded: 2 | Relevance: 1 | RetrRel: 0.588\n",
      "[4] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.576\n",
      "[5] Correctness: 5 | Grounded: 5 | Relevance: 5 | RetrRel: 0.665\n",
      "[6] Correctness: 3 | Grounded: 5 | Relevance: 5 | RetrRel: 0.704\n",
      "[7] Correctness: 2 | Grounded: 5 | Relevance: 2 | RetrRel: 0.646\n",
      "[8] Correctness: 4 | Grounded: None | Relevance: 5 | RetrRel: 0.674\n",
      "[9] Correctness: 2 | Grounded: 1 | Relevance: 5 | RetrRel: 0.639\n",
      "[10] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.698\n",
      "\n",
      "=== Evaluating method: step_back ===\n",
      "[1] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.723\n",
      "[2] Correctness: 5 | Grounded: 5 | Relevance: 5 | RetrRel: 0.638\n",
      "[3] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.668\n",
      "[4] Correctness: 4 | Grounded: None | Relevance: 5 | RetrRel: 0.598\n",
      "[5] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.665\n",
      "[6] Correctness: 3 | Grounded: 5 | Relevance: 5 | RetrRel: 0.730\n",
      "[7] Correctness: 2 | Grounded: 5 | Relevance: 2 | RetrRel: 0.651\n",
      "[8] Correctness: 2 | Grounded: 1 | Relevance: 2 | RetrRel: 0.674\n",
      "[9] Correctness: 1 | Grounded: 1 | Relevance: 5 | RetrRel: 0.624\n",
      "[10] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.729\n",
      "\n",
      "=== Evaluating method: hyde ===\n",
      "[1] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.712\n",
      "[2] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.638\n",
      "[3] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.668\n",
      "[4] Correctness: 4 | Grounded: 5 | Relevance: 5 | RetrRel: 0.579\n",
      "[5] Correctness: 4 | Grounded: None | Relevance: 5 | RetrRel: 0.665\n",
      "[6] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.715\n",
      "[7] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.637\n",
      "[8] Correctness: 2 | Grounded: 1 | Relevance: 2 | RetrRel: 0.674\n",
      "[9] Correctness: 2 | Grounded: 1 | Relevance: 5 | RetrRel: 0.622\n",
      "[10] Correctness: 2 | Grounded: 5 | Relevance: 5 | RetrRel: 0.729\n",
      "\n",
      "=== Combined Summary ===\n",
      "{\n",
      "  \"methods\": [\n",
      "    {\n",
      "      \"method\": \"multi_query\",\n",
      "      \"n\": 10,\n",
      "      \"avg_correctness_1to5\": 2.2,\n",
      "      \"avg_groundedness_1to5\": 2.0,\n",
      "      \"avg_relevance_1to5\": 2.9,\n",
      "      \"avg_retrieval_relevance_0to1\": 0.601,\n",
      "      \"generator_model\": \"llama3-8b-8192\",\n",
      "      \"judge_model\": \"llama3-8b-8192\"\n",
      "    },\n",
      "    {\n",
      "      \"method\": \"rag_fusion\",\n",
      "      \"n\": 10,\n",
      "      \"avg_correctness_1to5\": 3.1,\n",
      "      \"avg_groundedness_1to5\": 4.222,\n",
      "      \"avg_relevance_1to5\": 4.3,\n",
      "      \"avg_retrieval_relevance_0to1\": 0.665,\n",
      "      \"generator_model\": \"llama3-8b-8192\",\n",
      "      \"judge_model\": \"llama3-8b-8192\"\n",
      "    },\n",
      "    {\n",
      "      \"method\": \"decomposition\",\n",
      "      \"n\": 10,\n",
      "      \"avg_correctness_1to5\": 2.9,\n",
      "      \"avg_groundedness_1to5\": 3.889,\n",
      "      \"avg_relevance_1to5\": 4.0,\n",
      "      \"avg_retrieval_relevance_0to1\": 0.65,\n",
      "      \"generator_model\": \"llama3-8b-8192\",\n",
      "      \"judge_model\": \"llama3-8b-8192\"\n",
      "    },\n",
      "    {\n",
      "      \"method\": \"step_back\",\n",
      "      \"n\": 10,\n",
      "      \"avg_correctness_1to5\": 3.1,\n",
      "      \"avg_groundedness_1to5\": 4.111,\n",
      "      \"avg_relevance_1to5\": 4.4,\n",
      "      \"avg_retrieval_relevance_0to1\": 0.67,\n",
      "      \"generator_model\": \"llama3-8b-8192\",\n",
      "      \"judge_model\": \"llama3-8b-8192\"\n",
      "    },\n",
      "    {\n",
      "      \"method\": \"hyde\",\n",
      "      \"n\": 10,\n",
      "      \"avg_correctness_1to5\": 2.8,\n",
      "      \"avg_groundedness_1to5\": 4.111,\n",
      "      \"avg_relevance_1to5\": 4.7,\n",
      "      \"avg_retrieval_relevance_0to1\": 0.664,\n",
      "      \"generator_model\": \"llama3-8b-8192\",\n",
      "      \"judge_model\": \"llama3-8b-8192\"\n",
      "    }\n",
      "  ],\n",
      "  \"best_by_correctness\": {\n",
      "    \"method\": \"rag_fusion\",\n",
      "    \"n\": 10,\n",
      "    \"avg_correctness_1to5\": 3.1,\n",
      "    \"avg_groundedness_1to5\": 4.222,\n",
      "    \"avg_relevance_1to5\": 4.3,\n",
      "    \"avg_retrieval_relevance_0to1\": 0.665,\n",
      "    \"generator_model\": \"llama3-8b-8192\",\n",
      "    \"judge_model\": \"llama3-8b-8192\"\n",
      "  },\n",
      "  \"best_by_groundedness\": {\n",
      "    \"method\": \"rag_fusion\",\n",
      "    \"n\": 10,\n",
      "    \"avg_correctness_1to5\": 3.1,\n",
      "    \"avg_groundedness_1to5\": 4.222,\n",
      "    \"avg_relevance_1to5\": 4.3,\n",
      "    \"avg_retrieval_relevance_0to1\": 0.665,\n",
      "    \"generator_model\": \"llama3-8b-8192\",\n",
      "    \"judge_model\": \"llama3-8b-8192\"\n",
      "  },\n",
      "  \"best_by_relevance\": {\n",
      "    \"method\": \"hyde\",\n",
      "    \"n\": 10,\n",
      "    \"avg_correctness_1to5\": 2.8,\n",
      "    \"avg_groundedness_1to5\": 4.111,\n",
      "    \"avg_relevance_1to5\": 4.7,\n",
      "    \"avg_retrieval_relevance_0to1\": 0.664,\n",
      "    \"generator_model\": \"llama3-8b-8192\",\n",
      "    \"judge_model\": \"llama3-8b-8192\"\n",
      "  },\n",
      "  \"best_by_retrieval_relevance\": {\n",
      "    \"method\": \"step_back\",\n",
      "    \"n\": 10,\n",
      "    \"avg_correctness_1to5\": 3.1,\n",
      "    \"avg_groundedness_1to5\": 4.111,\n",
      "    \"avg_relevance_1to5\": 4.4,\n",
      "    \"avg_retrieval_relevance_0to1\": 0.67,\n",
      "    \"generator_model\": \"llama3-8b-8192\",\n",
      "    \"judge_model\": \"llama3-8b-8192\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
